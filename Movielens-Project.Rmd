---
title: "Movielens-Project"
author: "SusanSunny"
date: "12/31/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Suppress summarise info
options(dplyr.summarise.inform = FALSE)
```
## 1 Introduction

## 1.1 Project Aim 
The aim of this project is to build a movie recommendation system. The program should predict how a user will rate a certain movie based on the past ratings. 

## 1.2 Project Overview 
To complete this task several methods of Machine Learning will be used:

Firstly, linear regression is used to build the recommendation system. The variables included in the linear regression in my analysis are movie, user, genre and the year. 

Secondly, regularization is used to improve the results. Small sample sizes have a higher variability and the error is likely to be larger. Regularization is a method that penalizes estimates that come from small sample sizes with the aim of reducing the error. 

```{r creating test set, include=FALSE}
##########################################################
# Create edx set, validation set (final hold-out test set)
##########################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 3.6 or earlier:
#movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
#                                           title = as.character(title),
#                                           genres = as.character(genres))
# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

```

## 2 Methods and Analysis

## 2.1 Data exploration and visualization
Before doing an analysis, it is important to be familiar with the data set. In this chapter the data set is explored. 
The data set provided for this project is split into train and test set. The training set "edx"  contains 6 columns (userId, movieId, rating, timestamp, title and genres) and 9000055 rows. 
The test set validation contains 999999 rows, around 10% compared to the number of ratings in the training set. 
```{r exploration of movielens}
dim(edx)
head(edx)
dim(validation)
```
The year the movie was released is included in the title and was extracted using regular expressions. 
In the following plot, the number of movies per year can be seen. The number of movies is increasing steadily until around 1978, from then the number of movies is increasing even faster. 

```{r number of movies per year, echo = FALSE}
part<- edx%>%mutate(years=str_extract(title, "\\([0-9]{4}\\)"))%>%
  mutate(years=str_extract(years, "[0-9]{4}"))%>%
  group_by(years)%>%
  summarize(movies_per_year=n_distinct(movieId))%>%
  ungroup()
part%>%
  ggplot(aes(years,movies_per_year))+
  geom_point()+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 5))
``` 

The following plot shows the number of ratings per movie plotted against the year. Similarly to the plot above, the number of ratings per movie increases over the years until recent years. For recent movies, there are not as many ratings yet.  

```{r plot ratings per movie per year, echo=FALSE}
rpm<-edx%>%mutate(years=str_extract(title, "\\([0-9]{4}\\)"))%>%
  mutate(years=str_extract(years, "[0-9]{4}"))%>%
  group_by(movieId)%>%
  summarize(ratings_per_movie=n(), year=unique(years))
rpm%>%
  ggplot(aes(year, ratings_per_movie))+
  geom_point()+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size=5))
```

## 2.2 Linear Regression: Movie, User, Genre and Year Effect 

The basic concept of Machine Learning is to split a data set into training set and test set and to only use the training set to build the model, whereas the test set should only be used to test the model. 
In this project, the training set is called "edx" and the test set is called "validation".

The basic method of the model is linear regression with the following equation: pred = mu + b_i + b_u + b_g + b_y. (pred = predicted rating, b_i = movie effect, b_u = user effect, b_g = genre effect, b_y = year effect)

```{r Movie+User+Genre+Year Effect, warning=FALSE}
#Linear Regression: Movie + User + Year + Genre Effect Model 
mu <- mean(validation$rating) # overall average rating 

#Movie Effect 
b_i<- edx%>%
  group_by(movieId)%>%
  summarize(b_i = mean(rating-mu))

#User Effect
b_u<- edx%>%
  left_join(b_i, by = "movieId")%>%
  group_by(userId)%>%
  summarize(b_u = mean(rating-mu-b_i))

#Genre Effect
b_g<- edx%>%
  left_join(b_i, by = "movieId")%>%
  left_join(b_u, by = "userId")%>%
  group_by(genres)%>%
  summarize(b_g = mean(rating-mu-b_i-b_u))

#Year Effect (year in which the movie was released)
b_y<- edx%>%
  left_join(b_i, by = "movieId")%>%
  left_join(b_u, by = "userId")%>%
  left_join(b_g, by= "genres")%>%
  mutate(years=str_extract(title, "\\([0-9]{4}\\)"))%>%
  group_by(years)%>%
  summarize(b_y = mean(rating- mu - b_i - b_u - b_g))
```

There are several ways to evaluate a model. In this project, the root-mean-square error (RMSE) is used. For evaluating the model which is trained using only the training set, we use the test set "validation". 
In order to calculate the RMSE, the parameters (b_i, b_u, b_g, b_y) are joined into the test set "validation", so that the ratings can be predicted for the test set and can be compared to the true ratings of the test set. 
```{r validation}
#join b_i, b_u, b_g into validation set 
predicted_ratings<- validation%>%
  left_join(b_i, by = "movieId")%>%
  left_join(b_u, by = "userId")%>%
  left_join(b_g, by= "genres")%>%
  mutate(years=str_extract(title, "\\([0-9]{4}\\)"))%>%
  left_join(b_y, by= "years")%>%
  mutate(pred= mu + b_i + b_u + b_g + b_y)%>%
  .$pred 

#calculate RMSE 
iugy_rmse<-RMSE(predicted_ratings, validation$rating)
paste0("RMSE of Movie+User+Genre+Year Model: ", iugy_rmse)
```


## 2.4 Regularization 
Since there are a lot of movies that only have one or few ratings and their ratings have a high variability, they are more likely to result in a higher error when the algorithm is tested on the test set. With regularization, a penalty term is added to the objective function. The penalty term contains a variable lambda which is a tuning parameter. Thus, for the best result, the best value for the parameter has be calculated. 
In order to do so, the algorithm is applied on different values of lambda.  

```{r regularization}
##Use regularization
#calculate best lambda

lambdas<- seq(0,10,0.5) 

#build linear model depending on different lambdas 
rmses<- sapply(lambdas, function(lambda){  
  
  #Movie Effect
  b_i_reg<- edx%>%
    group_by(movieId)%>%
    summarize(b_i_reg = sum(rating-mu)/(n()+lambda)) 

  #User Effect
  b_u_reg<- edx%>%
    left_join(b_i_reg, by = "movieId")%>%
    group_by(userId)%>%
    summarize(b_u_reg = sum(rating-mu-b_i_reg)/(n()+lambda))

  #Genre Effect
  b_g_reg<- edx%>%
    left_join(b_i_reg, by = "movieId")%>%
    left_join(b_u_reg, by = "userId")%>%
    group_by(genres)%>%
    summarize(b_g_reg = sum(rating-mu-b_i_reg-b_u_reg)/(n()+lambda))

  #Year Effect (year in which the movie was released)
  b_y_reg<- edx%>%
    left_join(b_i_reg, by = "movieId")%>%
    left_join(b_u_reg, by = "userId")%>%
    left_join(b_g_reg, by= "genres")%>%
    mutate(years=str_extract(title, "\\([0-9]{4}\\)"))%>%
    group_by(years)%>%
    summarize(b_y_reg = sum(rating-mu-b_i_reg-b_u_reg-b_g_reg)/(n()+lambda))

  #join b_i, b_u, b_g into the validation set 
  predicted_ratings_reg<- validation%>%
    left_join(b_i_reg, by = "movieId")%>%
    left_join(b_u_reg, by = "userId")%>%
    left_join(b_g_reg, by= "genres")%>%
    mutate(years=str_extract(title, "\\([0-9]{4}\\)"))%>%
    left_join(b_y_reg, by= "years")%>%
    mutate(pred= mu + b_i_reg + b_u_reg + b_g_reg + b_y_reg)%>%
    .$pred 

  #calculate RMSE
  return(RMSE(predicted_ratings_reg, validation$rating))
})
```

In the following plot the RMSEs are plotted against the different values for lambda in order to find the best lambda.  

```{r plot lambdas, echo=FALSE}
plot(lambdas, rmses)
paste0("Lambda with best RMSE: ", lambdas[which.min(rmses)])
best_reg_rmse<-min(rmses)
paste0("RMSE of Final Model: ", best_reg_rmse)

```

## 3 Results
The following table shows the different RMSEs resulting when using the different models. Building a linear model with one or two variables (in this case Movie and User Effect) results in a substantial improvement of the RMSE. As expected, adding more variables (here year and genre) to the model only has a small effect on the RMSE and adding regularization can improve the RMSE further by a little bit. The RMSE of the final model is: 	0.8642930.
```{r summary table, echo=FALSE}
#generate RSMES for Movie, User, Genre, Years, Movie+User, Movie+User+Genre, Movie+User+Genre+Year
predicted_ratings<- validation%>%
  left_join(b_i, by = "movieId")%>%
  left_join(b_u, by = "userId")%>%
  left_join(b_g, by= "genres")%>%
  mutate(years=str_extract(title, "\\([0-9]{4}\\)"))%>%
  left_join(b_y, by= "years")%>%
  mutate(no = mu, i = mu + b_i)%>%
  mutate(iu= mu + b_i + b_u, iug= mu + b_i + b_u + b_g, iugy= mu+ b_i + b_u + b_g+ b_y)%>%
  dplyr::select(no, i, iu, iug, iugy)

#convert to Matrix 
prm<- as.matrix(predicted_ratings)
#calculate RSME
all_rmses<- apply(prm, 2, function(x) RMSE(validation$rating, x))
#sort RMSE and convert to data.table
all_rmses<- sort(all_rmses, decreasing=TRUE)
all_rmses_df<- data.table(Model=names(all_rmses), RMSE=all_rmses)


#add Models and RMSEs in table 
rmse_results<- data.frame(Model=c("Average Rating", "Movie Effect", "Movie+User Effect", "Movie+User+Genre Effect", "Movie+User+Genre+Year Effect", "Regularization+Movie+User+Genre+Year"), 
                        RMSE = c(all_rmses_df$RMSE, best_reg_rmse))
rmse_results %>% knitr::kable()
```


## 4 Conclusion 
This project uses only linear regression and regularization. However, there are more machine learning methods that could be used to improve the algorithm.